# -*- coding: utf-8 -*-
import pandas as pd
import re
from rapidfuzz import process, fuzz
import io
import streamlit as st
import os

# --- Configura√ß√µes Iniciais da P√°gina ---
st.set_page_config(
    page_title="Circuit Flow Completo",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CSS Simplificado para evitar TypeError e garantir alinhamento ---
st.markdown("""
<style>
/* For√ßa o alinhamento √† esquerda no campo de texto principal */
div[data-testid="stTextarea"] textarea {
    text-align: left !important;
    font-family: monospace;
    white-space: pre-wrap;
}
/* Alinha os t√≠tulos e outros elementos em geral */
h1, h2, h3, h4, .stMarkdown {
    text-align: left !important;
}
</style>
""", unsafe_html=True)
# --------------------------------------------------------------------------------------


# --- Configura√ß√µes Globais (Colunas) ---
COLUNA_ENDERECO = 'Destination Address'
COLUNA_SEQUENCE = 'Sequence'
COLUNA_LATITUDE = 'Latitude'
COLUNA_LONGITUDE = 'Longitude'
# NOVAS COLUNAS
COLUNA_GAIOLA = 'Gaiola' 
COLUNA_ID_UNICO = 'ID_UNICO' # ID tempor√°rio: Gaiola-Sequence (Ex: A1-1, G3-1)

# ===============================================
# FUN√á√ïES DE PR√â-ROTEIRIZA√á√ÉO (CORRE√á√ÉO/AGRUPAMENTO)
# ===============================================

def limpar_endereco(endereco):
    """
    Normaliza o texto do endere√ßo para melhor compara√ß√£o.
    MANT√âM N√öMEROS e V√çRGULAS (,) para que endere√ßos com n√∫meros diferentes
    n√£o sejam agrupados.
    """
    if pd.isna(endereco):
        return ""
    endereco = str(endereco).lower().strip()
    
    # Remove caracteres que N√ÉO s√£o alfanum√©ricos (\w), espa√ßo (\s) OU V√çRGULA (,)
    endereco = re.sub(r'[^\w\s,]', '', endereco) 
    
    # Substitui m√∫ltiplos espa√ßos por um √∫nico
    endereco = re.sub(r'\s+', ' ', endereco)
    
    # Substitui abrevia√ß√µes comuns para padroniza√ß√£o
    endereco = endereco.replace('rua', 'r').replace('avenida', 'av').replace('travessa', 'tr')
    
    return endereco


# Fun√ß√£o auxiliar para lidar com valores vazios no mode()
def get_most_common_or_empty(x):
    """
    Retorna o valor mais comum de uma S√©rie Pandas ou uma string vazia se todos forem NaN.
    """
    x_limpo = x.dropna()
    if x_limpo.empty:
        return ""
    return x_limpo.mode().iloc[0]


@st.cache_data
def processar_e_corrigir_dados(df_entrada, limite_similaridade):
    """
    Fun√ß√£o principal que aplica a corre√ß√£o e o agrupamento.
    """
    colunas_essenciais = [COLUNA_ENDERECO, COLUNA_SEQUENCE, COLUNA_LATITUDE, COLUNA_LONGITUDE, 'Bairro', 'City', 'Zipcode/Postal code', COLUNA_GAIOLA, COLUNA_ID_UNICO]
    for col in colunas_essenciais:
        if col not in df_entrada.columns:
            st.error(f"Erro: A coluna essencial '{col}' n√£o foi encontrada. Verifique se o DataFrame foi carregado corretamente.")
            return None, None 

    df = df_entrada.copy()
    
    # Preenchimento e Garantia de Tipos (Essencial)
    df['Bairro'] = df['Bairro'].astype(str).replace('nan', '', regex=False)
    df['City'] = df['City'].astype(str).replace('nan', '', regex=False)
    df['Zipcode/Postal code'] = df['Zipcode/Postal code'].astype(str).replace('nan', '', regex=False)
    df[COLUNA_GAIOLA] = df[COLUNA_GAIOLA].astype(str).replace('nan', '', regex=False)
    
    # Cria a coluna num√©rica para a ORDENA√á√ÉO. Aqui, usamos a SEQUENCE original do pacote.
    # A coluna ID_UNICO j√° deve vir com o * do volumoso, se houver.
    df['Sequence_Num'] = df[COLUNA_SEQUENCE].astype(str).str.replace(r'\*|\s', '', regex=True)
    df['Sequence_Num'] = pd.to_numeric(df['Sequence_Num'], errors='coerce').fillna(float('inf')).astype(float)


    # 1. Limpeza e Normaliza√ß√£o (Fuzzy Matching)
    df['Endereco_Limpo'] = df[COLUNA_ENDERECO].apply(limpar_endereco)
    enderecos_unicos = df['Endereco_Limpo'].unique()
    mapa_correcao = {}
    
    # 2. Fuzzy Matching para Agrupamento
    progresso_bar = st.progress(0, text="Iniciando Fuzzy Matching...")
    total_unicos = len(enderecos_unicos)
    
    if total_unicos > 0:
        for i, end_principal in enumerate(enderecos_unicos):
            if end_principal not in mapa_correcao:
                matches = process.extract(
                    end_principal, 
                    enderecos_unicos, 
                    scorer=fuzz.WRatio, 
                    limit=None
                )
                
                grupo_matches = [match[0] for match in matches if match[1] >= limite_similaridade]
                
                df_grupo = df[df['Endereco_Limpo'].isin(grupo_matches)]
                endereco_oficial_original = get_most_common_or_empty(df_grupo[COLUNA_ENDERECO])
                if not endereco_oficial_original:
                    endereco_oficial_original = end_principal 
                
                for end_similar in grupo_matches:
                    mapa_correcao[end_similar] = endereco_oficial_original
                    
                progresso_bar.progress((i + 1) / total_unicos, text=f"Processando {i+1} de {total_unicos} endere√ßos √∫nicos...")
        
        progresso_bar.empty()
        st.success("Fuzzy Matching conclu√≠do!")
    else:
        progresso_bar.empty()
        st.warning("Nenhum endere√ßo encontrado para processar.")


    # 3. Aplica√ß√£o do Endere√ßo Corrigido
    df['Endereco_Corrigido'] = df['Endereco_Limpo'].map(mapa_correcao)

    # 4. Agrupamento (POR ENDERE√áO CORRIGIDO E CIDADE)
    colunas_agrupamento = ['Endereco_Corrigido', 'City'] 
    
    df_agrupado = df.groupby(colunas_agrupamento).agg(
        # Agrupa os IDs √öNICOS (Gaiola-Sequence) que j√° cont√™m o '*'
        Sequences_Agrupadas=(COLUNA_ID_UNICO, 
                             lambda x: ','.join(map(str, sorted(x, key=lambda y: int(re.sub(r'[^\d]', '', str(y).split('-')[-1])) if re.sub(r'[^\d]', '', str(y).split('-')[-1]).isdigit() else float('inf'))))
                            ), 
        Total_Pacotes=('Sequence_Num', lambda x: (x != float('inf')).sum()), 
        Latitude=(COLUNA_LATITUDE, 'first'),
        Longitude=(COLUNA_LONGITUDE, 'first'),
        
        # Agrupa as informa√ß√µes comuns
        Bairro_Agrupado=('Bairro', get_most_common_or_empty),
        Zipcode_Agrupado=('Zipcode/Postal code', get_most_common_or_empty),
        
        # Agrupa as gaiolas (mant√©m TODAS as gaiolas √∫nicas daquele endere√ßo)
        Gaiola_Agrupada=(COLUNA_GAIOLA, lambda x: ','.join(sorted(x.unique()))),
        
        # Captura o menor n√∫mero de sequ√™ncia original (sem *) para ordena√ß√£o
        Min_Sequence=('Sequence_Num', 'min') 
        
    ).reset_index()

    # 5. ORDENA√á√ÉO: Ordena o DataFrame pelo menor n√∫mero de sequ√™ncia. (CRUCIAL!)
    df_agrupado = df_agrupado.sort_values(by='Min_Sequence').reset_index(drop=True)
    
    # 6. Formata√ß√£o do DF para o CIRCUIT 
    endereco_completo_circuit = (
        df_agrupado['Endereco_Corrigido'] + ', ' + 
        df_agrupado['Bairro_Agrupado'].str.strip() 
    )
    
    endereco_completo_circuit = endereco_completo_circuit.str.replace(r',\s*,', ',', regex=True)
    
    # Incluindo TODAS as Gaiolas nas Notas!
    notas_completas = (
        'Pacotes: ' + df_agrupado['Total_Pacotes'].astype(int).astype(str) + 
        ' | Gaiola(s): ' + df_agrupado['Gaiola_Agrupada'] +
        ' | Cidade: ' + df_agrupado['City'] + 
        ' | CEP: ' + df_agrupado['Zipcode_Agrupado']
    )

    df_circuit = pd.DataFrame({
        'Order ID': df_agrupado['Sequences_Agrupadas'], # IDs √öNICOS Agrupados (com *)
        'Address': endereco_completo_circuit, 
        'Latitude': df_agrupado['Latitude'], 
        'Longitude': df_agrupado['Longitude'], 
        'Notes': notas_completas
    })
    
    return df_circuit, df 


# ===============================================
# FUN√á√ïES DE P√ìS-ROTEIRIZA√á√ÉO (LIMPEZA P/ IMPRESS√ÉO)
# ===============================================

def extract_circuit_info(df_input_raw):
    """
    Processa o DataFrame da rota do Circuit (raw) para extrair o Order ID e Anota√ß√µes.
    """
    df = df_input_raw.copy()
    
    # 1. Padroniza√ß√£o de Colunas
    df.columns = df.columns.str.strip().str.lower()
    
    # Garante que colunas essenciais existam
    if 'notes' not in df.columns or '#' not in df.columns:
        raise KeyError("O arquivo da rota deve conter as colunas '#' (Sequ√™ncia de Parada) e 'Notes'.")

    # 2. Processa Notes para obter o ID agrupado (que pode ter o *)
    df['notes'] = df['notes'].astype(str).str.strip('"')
    df = df.dropna(subset=['notes']) 
    
    # Divide a coluna na primeira ocorr√™ncia de ';'
    df_split = df['notes'].str.split(';', n=1, expand=True)
    # O ID agrupado (ex: "A1-1,G3-2*") √© a primeira parte.
    df['Ordem ID'] = df_split[0].str.strip().str.strip('"') 
    
    # Anota√ß√µes completas (o resto da string)
    df['Anota√ß√µes Completas'] = df_split[1].str.strip() if 1 in df_split.columns else ""
    
    # 3. Formata a Lista de Impress√£o
    df['Lista de Impress√£o'] = (
        df['Ordem ID'].astype(str) + 
        ' - ' + 
        df['Anota√ß√µes Completas'].astype(str)
    )
    
    return df

# ===============================================
# INTERFACE PRINCIPAL
# ===============================================

st.title("üó∫Ô∏è Flow Completo Circuit (Pr√© e P√≥s-Roteiriza√ß√£o)")

# CRIA√á√ÉO DAS ABAS 
tab1, tab2 = st.tabs(["üöÄ Pr√©-Roteiriza√ß√£o (Importa√ß√£o)", "üìã P√≥s-Roteiriza√ß√£o (Impress√£o/C√≥pia)"])


# ----------------------------------------------------------------------------------
# ABA 1: PR√â-ROTEIRIZA√á√ÉO (CORRE√á√ÉO E IMPORTA√á√ÉO)
# ----------------------------------------------------------------------------------

with tab1:
    st.header("1. Gerar Arquivo para Importar no Circuit")
    st.caption("Esta etapa unifica rotas de diferentes gaiolas, corrige erros de digita√ß√£o, marca volumes e agrupa pedidos.")

    # Inicializa o estado para armazenar a lista de DataFrames carregados (com gaiola e nome)
    if 'loaded_dfs' not in st.session_state:
        st.session_state['loaded_dfs'] = []
    
    st.markdown("---")
    st.subheader("1.1 Carregar Planilhas Originais e Definir Gaiolas")
    st.info("Carregue **todas** as planilhas. A marca√ß√£o de volumosos ser√° feita para cada planilha individualmente na pr√≥xima etapa.")

    uploaded_files_pre = st.file_uploader(
        "Arraste e solte os arquivos originais (CSV/Excel) aqui:", 
        type=['csv', 'xlsx'],
        accept_multiple_files=True, 
        key="file_pre"
    )

    
    if uploaded_files_pre:
        # Verifica se a lista de arquivos mudou, se sim, limpa o estado
        current_file_names = {f.name for f in uploaded_files_pre}
        loaded_file_names = {item['file_name'] for item in st.session_state['loaded_dfs']}

        if current_file_names != loaded_file_names:
            st.session_state['loaded_dfs'] = []
            
            # Inicializa o estado com os novos arquivos
            for i, uploaded_file in enumerate(uploaded_files_pre):
                 # Adiciona um placeholder para a gaiola
                st.session_state['loaded_dfs'].append({
                    'file_name': uploaded_file.name,
                    'file_object': uploaded_file,
                    'gaiola': f"G{i+1}", 
                    'df': None, # O DataFrame bruto
                    'volumosos': set() # Set de Sequences originais marcadas como volumosas
                })
            
            # Limpa qualquer input antigo
            st.session_state['df_unificado_final'] = None

        
        st.markdown("#### Defina o C√≥digo da Gaiola para cada arquivo e Inicie a Carga:")
        
        # Usa um form para submeter todas as entradas de gaiola e iniciar a carga
        with st.form("gaiola_form"):
            for i, item in enumerate(st.session_state['loaded_dfs']):
                gaiola_input = st.text_input(
                    f"C√≥digo da Gaiola para **{item['file_name']}**", 
                    key=f"gaiola_input_{i}",
                    value=item['gaiola'],
                    max_chars=10
                )
                # Atualiza o item na lista com o valor digitado (temporariamente)
                item['gaiola'] = gaiola_input
                
            submitted = st.form_submit_button("Confirmar Gaiolas e Iniciar Processamento Individual")
            
            if submitted: 
                df_list = []
                gaiolas_ok = True
                
                st.markdown("---")
                st.subheader("Processando Arquivos Individualmente...")

                for i, item in enumerate(st.session_state['loaded_dfs']):
                    gaiola_code = item['gaiola'].strip()
                    uploaded_file = item['file_object']
                    
                    if not gaiola_code:
                        st.warning(f"O arquivo '{uploaded_file.name}' n√£o tem c√≥digo de gaiola definido. Por favor, preencha.")
                        gaiolas_ok = False
                        break

                    try:
                        # 1. Carregar DataFrame
                        if uploaded_file.name.endswith('.csv'):
                            df_input_pre = pd.read_csv(uploaded_file, encoding='utf-8')
                        else:
                            df_input_pre = pd.read_excel(uploaded_file, sheet_name=0)
                        
                        # 2. Valida√ß√£o e Prepara√ß√£o
                        colunas_basicas = [COLUNA_ENDERECO, COLUNA_SEQUENCE, COLUNA_LATITUDE, COLUNA_LONGITUDE, 'Bairro', 'City', 'Zipcode/Postal code']
                        for col in colunas_basicas:
                            if col not in df_input_pre.columns:
                                raise KeyError(f"A coluna '{col}' est√° faltando no arquivo '{uploaded_file.name}'.")
                        
                        # 3. Adicionar Gaiola e Coluna de Sequ√™ncia
                        df_input_pre[COLUNA_GAIOLA] = gaiola_code
                        df_input_pre[COLUNA_SEQUENCE] = df_input_pre[COLUNA_SEQUENCE].astype(str) # Garante que a sequ√™ncia √© string
                        
                        # 4. Salvar o DF processado (com a coluna Gaiola) no estado
                        item['df'] = df_input_pre.copy() 
                        st.session_state['loaded_dfs'][i] = item # Atualiza o item no state

                        st.info(f"‚úÖ Arquivo **{uploaded_file.name}** (Gaiola: **{gaiola_code}**) carregado com **{len(df_input_pre)}** pacotes.")
                        
                    except KeyError as ke:
                        st.error(f"Erro de Coluna no arquivo '{uploaded_file.name}': {ke}")
                        gaiolas_ok = False
                        break
                    except Exception as e:
                        st.error(f"Erro ao carregar o arquivo '{uploaded_file.name}'. Erro: {e}")
                        gaiolas_ok = False
                        break

                if gaiolas_ok:
                    st.success("Carga dos arquivos conclu√≠da. Prossiga para a marca√ß√£o de volumosos.")
                    st.session_state['df_unificado_final'] = None # Limpa o resultado final para for√ßar o rec√°lculo
                else:
                    st.session_state['loaded_dfs'] = [] # Se falhar, reseta a lista de arquivos carregados
                    st.session_state['df_unificado_final'] = None


    # Limpa a sess√£o se o arquivo for removido
    elif uploaded_files_pre is None and st.session_state.get('loaded_dfs'):
        st.session_state['loaded_dfs'] = []
        st.session_state['df_unificado_final'] = None
        st.rerun() 
        

    
    # ----------------------------------------------------------------------------------
    # 1.2 MARCA√á√ÉO INDIVIDUAL DE VOLUMOSOS
    # ----------------------------------------------------------------------------------
    
    # Verifica se h√° DFs carregados e prontos para a marca√ß√£o
    dfs_prontos_para_marcar = [item for item in st.session_state['loaded_dfs'] if item.get('df') is not None]
    
    if dfs_prontos_para_marcar:
        st.markdown("---")
        st.subheader("1.2 Marcar Pacotes Volumosos por Planilha (Volumosos = *)")
        st.info("Marque individualmente os n√∫meros de sequ√™ncia (Sequence) que s√£o volumosos em cada gaiola. Eles ser√£o marcados com `*`.")
        
        # Cria as abas para cada arquivo carregado
        tab_titles = [f"{item['gaiola']} ({item['file_name']})" for item in dfs_prontos_para_marcar]
        tabs = st.tabs(tab_titles)

        # Itera sobre os DFs prontos e cria a interface de marca√ß√£o
        for i, item in enumerate(dfs_prontos_para_marcar):
            with tabs[i]:
                df_current = item['df'].copy()
                gaiola_code = item['gaiola']
                
                st.markdown(f"#### Gaiola **{gaiola_code}** (Total de **{len(df_current)}** pacotes)")
                
                # --- Prepara√ß√£o da lista de Sequences Originais ---
                df_current['Sort_Key'] = pd.to_numeric(df_current[COLUNA_SEQUENCE], errors='coerce').fillna(float('inf'))
                sequences_sorted = df_current.sort_values('Sort_Key')[COLUNA_SEQUENCE].astype(str).unique()
                
                # Armazena os IDs volumosos (Sequences originais) desta gaiola no state
                volumosos_set = item['volumosos']
                
                # Callback para o checkbox
                def update_volumoso_set(seq_id, is_checked, item_index):
                    # Usamos o `session_state` diretamente
                    if is_checked:
                        st.session_state['loaded_dfs'][item_index]['volumosos'].add(seq_id)
                    elif seq_id in st.session_state['loaded_dfs'][item_index]['volumosos']:
                        st.session_state['loaded_dfs'][item_index]['volumosos'].remove(seq_id)

                # Colunas para o layout de faixa
                col_start, col_end, col_button_mark, col_button_unmark = st.columns([1.5, 1.5, 2, 2])
                
                # --- Marca√ß√£o em Faixa ---
                # Garante valores padr√£o se a lista de sequ√™ncias for vazia
                start_default = sequences_sorted[0] if len(sequences_sorted) > 0 else "1"
                end_default = sequences_sorted[-1] if len(sequences_sorted) > 0 else "1"
                
                with col_start:
                    start_seq = st.text_input(f"In√≠cio da Faixa (Seq)", value=start_default, key=f"start_seq_vol_{i}")
                with col_end:
                    end_seq = st.text_input(f"Fim da Faixa (Seq)", value=end_default, key=f"end_seq_vol_{i}")
                
                
                # Fun√ß√£o de helper para encontrar sequ√™ncias num√©ricas entre o range (mesmo que sejam strings)
                def get_sequences_in_range(df, col, start, end):
                    # Tenta converter para num√©rico para a compara√ß√£o de faixa
                    df['Temp_Num'] = pd.to_numeric(df[col], errors='coerce')
                    try:
                        start_num = pd.to_numeric(start, errors='coerce')
                        end_num = pd.to_numeric(end, errors='coerce')
                    except:
                        return []
                    
                    if pd.isna(start_num) or pd.isna(end_num): return []
                    
                    return df[
                        (df['Temp_Num'] >= start_num) & (df['Temp_Num'] <= end_num)
                    ][col].astype(str).unique().tolist()
                    
                
                with col_button_mark:
                    if st.button("Marcar Faixa", key=f"btn_mark_range_{i}"):
                        sequences_to_mark = get_sequences_in_range(df_current, COLUNA_SEQUENCE, start_seq, end_seq)
                        for seq in sequences_to_mark:
                            st.session_state['loaded_dfs'][i]['volumosos'].add(seq)
                        st.rerun() # Adiciona rerun para atualizar a contagem de volumosos no info

                with col_button_unmark:
                    if st.button("Limpar Faixa", key=f"btn_unmark_range_{i}"):
                        sequences_to_unmark = get_sequences_in_range(df_current, COLUNA_SEQUENCE, start_seq, end_seq)
                        for seq in sequences_to_unmark:
                            if seq in st.session_state['loaded_dfs'][i]['volumosos']:
                                st.session_state['loaded_dfs'][i]['volumosos'].remove(seq)
                        st.rerun() # Adiciona rerun para atualizar a contagem de volumosos no info

                st.info(f"**{len(volumosos_set)}** de **{len(sequences_sorted)}** pacotes marcados como volumosos nesta gaiola.")
                
                st.markdown("##### Marca√ß√£o Individual")
                with st.container(height=250):
                    # Marca√ß√£o individual por checkbox
                    for seq_id in sequences_sorted:
                        is_checked = seq_id in volumosos_set
                        st.checkbox(
                            f"Seq: {seq_id}", 
                            value=is_checked, 
                            key=f"vol_{gaiola_code}_{seq_id}",
                            on_change=update_volumoso_set, 
                            args=(seq_id, not is_checked, i) 
                        )
        
        # ----------------------------------------------------------------------------------
        # 1.3 UNIFICA√á√ÉO, CORRE√á√ÉO E PROCESSAMENTO FINAL
        # ----------------------------------------------------------------------------------
        st.markdown("---")
        st.subheader("1.3 Unificar e Processar Rotas")
        
        limite_similaridade_ajustado = st.slider(
            'Ajuste a Precis√£o do Corretor (Fuzzy Matching):',
            min_value=80,
            max_value=100,
            value=100, 
            step=1,
            help="Use 100% para garantir que endere√ßos na mesma rua com n√∫meros diferentes n√£o sejam agrupados (recomendado)."
        )
        st.info(f"O limite de similaridade est√° em **{limite_similaridade_ajustado}%**.")
        
        
        if st.button("üöÄ UNIFICAR, CORRIGIR E AGRUPAR PARA CIRCUIT", key="btn_pre_final_run"):
            
            df_final_list = []
            
            # 1. Unifica√ß√£o e Aplica√ß√£o do Asterisco (*) e ID_UNICO
            for item in st.session_state['loaded_dfs']:
                # Pula se o DF n√£o foi carregado corretamente
                if item['df'] is None:
                    continue
                    
                df_proc = item['df'].copy()
                gaiola = item['gaiola']
                volumosos = item['volumosos']
                
                # Cria a coluna ID_UNICO sem o asterisco inicial
                df_proc[COLUNA_ID_UNICO] = df_proc[COLUNA_GAIOLA].astype(str) + '-' + df_proc[COLUNA_SEQUENCE].astype(str)

                # Aplica o * (asterisco) no ID_UNICO se a Sequence original estiver no set de volumosos
                for seq_volumoso in volumosos:
                    str_seq_volumoso = str(seq_volumoso)
                    
                    # Filtra os registros que correspondem √†quela SEQUENCE e GAIOLA
                    df_proc.loc[
                        (df_proc[COLUNA_SEQUENCE] == str_seq_volumoso) & (df_proc[COLUNA_GAIOLA] == gaiola), 
                        COLUNA_ID_UNICO
                    ] = df_proc[COLUNA_ID_UNICO] + '*'

                df_final_list.append(df_proc)
                
            if not df_final_list:
                st.error("N√£o h√° planilhas v√°lidas e processadas para unificar. Carregue os arquivos e clique em 'Confirmar Gaiolas'.")
                # Se falhar aqui, n√£o prossegue
            else:
                # CONCATENA√á√ÉO FINAL: Junta todos os DataFrames (com ID_UNICO j√° marcado)
                df_unificado = pd.concat(df_final_list, ignore_index=True)
                st.session_state['df_unificado_final'] = df_unificado.copy()
                
                # 2. Iniciar o processamento e agrupamento (Fuzzy Matching, Agrupamento e Ordena√ß√£o)
                df_circuit, df_processado_completo = processar_e_corrigir_dados(
                    st.session_state['df_unificado_final'], 
                    limite_similaridade_ajustado
                )
                
                if df_circuit is not None:
                    st.markdown("---")
                    st.header("‚úÖ Resultado Conclu√≠do!")
                    
                    total_entradas = len(st.session_state['df_unificado_final'])
                    total_agrupados = len(df_circuit)
                    
                    st.metric(
                        label="Endere√ßos √önicos Agrupados",
                        value=total_agrupados,
                        delta=f"-{total_entradas - total_agrupados} pacotes agrupados"
                    )
                    
                    # --- SA√çDA 1: ARQUIVO PARA CIRCUIT (ROTEIRIZA√á√ÉO) ---
                    st.subheader("Arquivo para Roteiriza√ß√£o (Circuit)")
                    st.dataframe(df_circuit, use_container_width=True)
                    
                    buffer_circuit = io.BytesIO()
                    with pd.ExcelWriter(buffer_circuit, engine='openpyxl') as writer:
                        df_circuit.to_excel(writer, index=False, sheet_name='Circuit Import')
                    buffer_circuit.seek(0)
                    
                    st.download_button(
                        label="üì• Baixar ARQUIVO PARA CIRCUIT",
                        data=buffer_circuit,
                        file_name="Circuit_Import_FINAL_MARCADO.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="download_excel_circuit"
                    )
                    
                    # --- SA√çDA 2: PLANILHA DE VOLUMOSOS SEPARADA ---
                    df_volumosos = df_processado_completo[
                        df_processado_completo[COLUNA_ID_UNICO].astype(str).str.contains(r'\*', regex=True, na=False)
                    ].copy()
                    
                    df_volumosos['Sort_Key'] = df_volumosos[COLUNA_SEQUENCE].astype(str).str.replace(r'\*|\s', '', regex=True)
                    df_volumosos['Sort_Key'] = pd.to_numeric(df_volumosos['Sort_Key'], errors='coerce')
                    df_volumosos = df_volumosos.sort_values(by=['Gaiola', 'Sort_Key']).drop(columns=['Sort_Key'])

                    if not df_volumosos.empty:
                        st.markdown("---")
                        st.subheader("Planilha de APENAS Volumosos (Pacotes com *)")
                        st.caption(f"Cont√©m **{len(df_volumosos)}** itens marcados com *. Ordenado por Gaiola e Sequ√™ncia Original.")

                        df_vol_export = df_volumosos[[
                            COLUNA_ID_UNICO, 
                            COLUNA_GAIOLA, 
                            COLUNA_SEQUENCE, 
                            COLUNA_ENDERECO, 
                            'Bairro', 
                            'City', 
                            'Zipcode/Postal code',
                            'Endereco_Corrigido'
                        ]].copy()
                        
                        df_vol_export.columns = [
                            'ID √önico (Gaiola-Seq*)', 
                            'Gaiola',
                            'N¬∫ da Sequ√™ncia Original',
                            'Endere√ßo Original', 
                            'Bairro', 
                            'Cidade', 
                            'CEP', 
                            'Endere√ßo Corrigido/Agrupado'
                        ]

                        st.dataframe(df_vol_export, use_container_width=True)
                        
                        buffer_vol = io.BytesIO()
                        with pd.ExcelWriter(buffer_vol, engine='openpyxl') as writer:
                            df_vol_export.to_excel(writer, index=False, sheet_name='Volumosos')
                        buffer_vol.seek(0)
                        
                        st.download_button()
                            label="üì• Baixar PLANILHA APENAS VOLUMOSOS",
                            data

