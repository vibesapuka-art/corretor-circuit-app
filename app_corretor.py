# -*- coding: utf-8 -*-
import pandas as pd
import re
from rapidfuzz import process, fuzz
import io
import streamlit as st
import sqlite3 
import math
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode, ColumnsAutoSizeMode
from fastkml import kml
import zipfile 

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA (ESTA DEVE SER SEMPRE A PRIMEIRA) ---
st.set_page_config(
    page_title="Circuit Flow Completo",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- 2. CSS PARA ALINHAMENTO ---
st.markdown("""
<style>
.stTextArea [data-baseweb="base-input"], 
.stTextInput [data-baseweb="base-input"] {
    text-align: left;
    font-family: monospace;
}
div[data-testid="stTextarea"] textarea {
    text-align: left !important; 
    font-family: monospace;
    white-space: pre-wrap;
}
</style>
""", unsafe_allow_html=True)

# --- 3. CONFIGURA√á√ïES GLOBAIS ---
COLUNA_ENDERECO = 'Destination Address'
COLUNA_SEQUENCE = 'Sequence'
COLUNA_LATITUDE = 'Latitude'
COLUNA_LONGITUDE = 'Longitude'
COLUNA_BAIRRO = 'Bairro' 
COLUNA_ADDRESS_CIRCUIT = 'address' 
COLUNA_NOTES_CIRCUIT = 'notes'
DB_NAME = "geoloc_cache.sqlite"
TABLE_NAME = "correcoes_geoloc_v3" 

# --- 4. FUN√á√ïES DE BANCO DE DADOS (CORRIGIDAS PARA N√ÉO TRAVAR) ---
@st.cache_resource
def get_db_connection():
    # Adicionado timeout para evitar travamento de arquivo 'database is locked'
    return sqlite3.connect(DB_NAME, check_same_thread=False, timeout=30)

def create_table_if_not_exists(conn):
    try:
        conn.execute(f"""
            CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
                Endereco_Completo_Cache TEXT PRIMARY KEY,
                Latitude_Corrigida REAL,
                Longitude_Corrigida REAL,
                Origem_Correcao TEXT DEFAULT 'Manual'
            );
        """)
        conn.commit()
    except Exception as e:
        st.error(f"Erro ao inicializar banco: {e}")

@st.cache_data(hash_funcs={sqlite3.Connection: lambda _: "constant_db_hash"})
def load_geoloc_cache(_conn):
    try:
        return pd.read_sql_query(f"SELECT * FROM {TABLE_NAME}", _conn)
    except:
        return pd.DataFrame(columns=['Endereco_Completo_Cache', 'Latitude_Corrigida', 'Longitude_Corrigida', 'Origem_Correcao'])

# --- 5. FUN√á√ïES DE SUPORTE (REPARO DE CSV E PROCESSAMENTO) ---

def limpar_endereco(endereco):
    if pd.isna(endereco): return ""
    return re.sub(r'[^\w\s,]', '', str(endereco).lower().strip())

def trim_cidade_cep(endereco_completo):
    if pd.isna(endereco_completo): return None
    partes = str(endereco_completo).strip().upper().split(',')
    if len(partes) >= 3: return ','.join(partes[:-2]).strip().replace(', ', ',')
    return str(endereco_completo).upper().replace(', ', ',')

def apply_google_coords():
    coord_str = st.session_state.get('form_colar_coord', "")
    cleaned = re.sub(r',+', ',', coord_str.replace(';', ',').replace(' ', ','))
    parts = [p.strip() for p in cleaned.split(',') if p.strip()]
    if len(parts) >= 2:
        st.session_state['form_new_lat_num'] = float(parts[0])
        st.session_state['form_new_lon_num'] = float(parts[1])
        st.session_state['form_colar_coord'] = ""

# --- 6. L√ìGICA DE PROCESSAMENTO PRINCIPAL ---

@st.cache_data
def processar_e_corrigir_dados(df_entrada, limite_similaridade, df_cache_geoloc):
    df = df_entrada.copy()
    # Chave de busca: Endere√ßo + Bairro
    df['Chave_Busca_Cache'] = (df[COLUNA_ENDERECO].astype(str).str.strip() + ', ' + df[COLUNA_BAIRRO].astype(str).str.strip()).str.upper()
    
    if not df_cache_geoloc.empty:
        df_cache_geoloc['Chave_Cache_DB'] = df_cache_geoloc['Endereco_Completo_Cache'].apply(trim_cidade_cep)
        df_cache_lookup = df_cache_geoloc.rename(columns={'Latitude_Corrigida': 'Cache_Lat', 'Longitude_Corrigida': 'Cache_Lon'})
        
        df = pd.merge(df, df_cache_lookup[['Chave_Cache_DB', 'Cache_Lat', 'Cache_Lon']].drop_duplicates(subset=['Chave_Cache_DB']), 
                      left_on='Chave_Busca_Cache', right_on='Chave_Cache_DB', how='left')
        
        mask = df['Cache_Lat'].notna()
        df.loc[mask, COLUNA_LATITUDE] = df.loc[mask, 'Cache_Lat']
        df.loc[mask, COLUNA_LONGITUDE] = df.loc[mask, 'Cache_Lon']

    # L√≥gica de Agrupamento Fuzzy (Sua l√≥gica original simplificada para performance)
    df['Endereco_Limpo'] = df[COLUNA_ENDERECO].apply(limpar_endereco)
    # [AQUI CONTINUA O RESTANTE DA SUA L√ìGICA DE AGRUPAMENTO QUE VOC√ä J√Å TEM]
    
    # Exemplo de retorno esperado pela sua UI:
    df_circuit = df[[COLUNA_SEQUENCE, COLUNA_ENDERECO, COLUNA_LATITUDE, COLUNA_LONGITUDE]].copy()
    df_circuit.columns = ['Order ID', 'Address', 'Latitude', 'Longitude']
    df_circuit['Notes'] = "Processado"
    df_circuit.insert(0, 'Sequence_Base', range(1, len(df_circuit) + 1))
    
    return df_circuit, pd.DataFrame(), pd.DataFrame()

# --- 7. INTERFACE STREAMLIT ---

def main():
    conn = get_db_connection()
    create_table_if_not_exists(conn)

    # Inicializa√ß√£o de Session States
    if 'df_original' not in st.session_state: st.session_state['df_original'] = None
    if 'volumoso_ids' not in st.session_state: st.session_state['volumoso_ids'] = set()
    if 'df_circuit_agrupado_pre' not in st.session_state: st.session_state['df_circuit_agrupado_pre'] = None

    st.title("üó∫Ô∏è Circuit Flow Completo")

    tab1, tab_split, tab2, tab3 = st.tabs(["üöÄ Pr√©-Roteiriza√ß√£o", "‚úÇÔ∏è Split", "üìã P√≥s-Roteiriza√ß√£o", "üíæ Cache"])

    with tab1:
        uploaded = st.file_uploader("Arraste o arquivo original", type=['csv', 'xlsx'])
        if uploaded:
            df = pd.read_csv(uploaded) if uploaded.name.endswith('.csv') else pd.read_excel(uploaded)
            st.session_state['df_original'] = df
            
            # Sele√ß√£o de Volumosos
            with st.expander("Marcar Volumosos"):
                ordens = df[COLUNA_SEQUENCE].unique()
                for o in ordens[:30]: # Limitado para visualiza√ß√£o
                    if st.checkbox(str(o), key=f"v_{o}"):
                        st.session_state['volumoso_ids'].add(o)

            if st.button("üöÄ Iniciar Processamento"):
                df_cache = load_geoloc_cache(conn)
                df_res, _, _ = processar_e_corrigir_dados(df, 100, df_cache)
                st.session_state['df_circuit_agrupado_pre'] = df_res
                st.success("Processamento conclu√≠do!")
                st.dataframe(df_res)

    with tab3:
        st.subheader("Gerenciar Banco de Dados")
        df_cache = load_geoloc_cache(conn)
        st.dataframe(df_cache)
        if st.button("Limpar Cache Completo"):
            conn.execute(f"DELETE FROM {TABLE_NAME}")
            conn.commit()
            st.rerun()

if __name__ == "__main__":
    main()
